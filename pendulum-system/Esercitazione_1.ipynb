{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b16416d",
   "metadata": {},
   "source": [
    "# Controlling Neural Networks with Rule Representations\n",
    "## This notebook is derived from this paper by Seo et al.\n",
    "https://arxiv.org/pdf/2106.07804.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "092d702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.distributions.beta import Beta\n",
    "\n",
    "from utils_dp import DoublePendulum, calc_double_E, verification\n",
    "\n",
    "from model import RuleEncoder, DataEncoder, Net, NaiveModel, SharedNet, DataonlyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9d6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {'ruleonly': {},\n",
    "              'dataonly': {},\n",
    "              'dataonly-nobatch': {},\n",
    "              'dataonly-nobatch-constraint1.0': {'constraint': 1.0},\n",
    "              'dataonly-nobatch-constraint0.1': {'constraint': 0.1},\n",
    "              'dataonly-nobatch-constraint0.01': {'constraint': 0.01},\n",
    "              'paper': {'beta': [0.1], 'scale': 0, 'shared': True},\n",
    "              'dnn-crr': {'beta': [0.1], 'scale': 1, 'shared': True},\n",
    "              'dnn-crr-autoscale': {'beta': [0.1], 'scale': 0, 'shared': True}\n",
    "             }\n",
    "\n",
    "if not (os.path.exists(\"saved_models/\")):\n",
    "    os.makedirs(\"saved_models\")\n",
    "    print(\"created saved_model folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384ec3f8",
   "metadata": {},
   "source": [
    "### Parameter of the experiment\n",
    "#### Pendulum\n",
    "L1 : Rod length 1, M1 : Mass 1\n",
    "L2 : Rod length 2, M2 : Mass 2\n",
    "\n",
    "Friction coefficients F1, F2\n",
    "\n",
    "#### Initial condition for the  angles $\\theta_1$ and  $\\theta_2$ of the two pendulums are randomly sampled. Initial velocity is zero\n",
    "\n",
    "![pendulum image](https://upload.wikimedia.org/wikipedia/commons/7/78/Double-Pendulum.svg )\n",
    "#### Paramenter for the simulation\n",
    "delta_t : time step for the simulation\n",
    "tmax : max time\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210195a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1, L2, M1, M2, F1, F2 = 1, 1, 1, 5, 0.001, 0.001\n",
    "    \n",
    "init_theta1 = round(np.random.uniform(-np.pi/4, np.pi/4), 4)\n",
    "init_omega1 = 0.0\n",
    "init_theta2 = round(np.random.uniform(-np.pi/4, np.pi/4), 4)\n",
    "init_omega2 = 0.0\n",
    "\n",
    "tmax, dt = 3000, 0.005\n",
    "\n",
    "dp = DoublePendulum(L1, L2, M1, M2, init_theta1, init_omega1, init_theta2, init_omega2, F1, F2)\n",
    "\n",
    "device = 'cuda:0'\n",
    "seed = 42\n",
    "\n",
    "# task paramenters\n",
    "std_noise = 0.03  # noise added to trajectories\n",
    "sampling_step = 30    # sample a row for every the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131491c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, y = dp.generate(tmax=tmax, dt=dt)\n",
    "\n",
    "dp_params = {'M1': dp.M1, 'M2': dp.M2, 'L1': dp.L1, 'L2': dp.L2, 'g': dp.g, 'F1': dp.F1, 'F2': dp.F2}\n",
    "print('sequence length: {} ({} sec)'.format(len(y), tmax))\n",
    "print('dt: {} (sec)\\n'.format(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce184e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ac114",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_time = np.arange(y.shape[0])*dt\n",
    "fig, ax = plt.subplots(figsize=[10, 6])\n",
    "ax.plot(x_time[1000:5000],y[1000:5000,0],label = r\"$\\theta_1$\")\n",
    "ax.plot(x_time[1000:5000],y[1000:5000,2],label = r\"$\\theta_2$\")\n",
    "plt.legend(fontsize=20)\n",
    "ax.set_ylabel('radiants', fontsize=24)\n",
    "ax.set_xlabel('time (s)', fontsize=24)\n",
    "\n",
    "_, _, pred_E = calc_double_E(y[1000:5000,:], **dp_params) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=[10, 6])\n",
    "ax.plot(x_time[1000:5000],pred_E,label = r\"Energy\")\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "ax.set_ylabel('J', fontsize=24)\n",
    "ax.set_xlabel('time (s)', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be900f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine dt for generation and subsample for learning\n",
    "\n",
    "sampling_dt = dt*sampling_step\n",
    "sampling_ind = np.arange(0, t.shape[0] - 1, sampling_step)\n",
    "sampling_t = t[sampling_ind]\n",
    "\n",
    "input_output_y = np.concatenate((y[:-1], y[1:]), axis=1)    # [[input, output]]\n",
    "X = input_output_y[sampling_ind]\n",
    "print('subsampled sequence length: {} ({} sec)'.format(len(X), tmax))\n",
    "print('sampling dt: {} (sec)'.format(sampling_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[10, 6])\n",
    "ax.plot( np.arange(X[1000:1100,0].shape[0])*sampling_dt,X[1000:1100,0],\"o\",label = r\"$\\theta_1$\")\n",
    "ax.plot( np.arange(X[1000:1100,0].shape[0])*sampling_dt,X[1000:1100,2],\"o\",label = r\"$\\theta_2$\")\n",
    "plt.legend(fontsize=20)\n",
    "ax.set_ylabel('radiants', fontsize=24)\n",
    "ax.set_xlabel('time (s)', fontsize=24)\n",
    "plt.title(\"Subsampled trajectories\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b194422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "X = torch.tensor(np.array(X), dtype=torch.float32, device=device)\n",
    "num_samples = X.shape[0]\n",
    "input_dim = X.shape[1]//2    # (theta1, omega1, theta2, omega2)\n",
    "\n",
    "# 60:10:30 split\n",
    "train_X, train_y = X[:int(num_samples*0.6), :input_dim], X[:int(num_samples*0.6), input_dim:]\n",
    "valid_X, valid_y = X[int(num_samples*0.6):int(num_samples*0.7), :input_dim], X[int(num_samples*0.6):int(num_samples*0.7), input_dim:]\n",
    "test_X, test_y = X[int(num_samples*0.7):, :input_dim], X[int(num_samples*0.7):, input_dim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266406b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_sample = len(train_X)\n",
    "total_valid_sample = len(valid_X)\n",
    "total_test_sample = len(test_X)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(train_X, train_y), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(valid_X, valid_y), batch_size=valid_X.shape[0])\n",
    "test_loader = DataLoader(TensorDataset(test_X, test_y), batch_size=test_X.shape[0])\n",
    "\n",
    "print(\"data size: {}/{}/{}\".format(len(train_X), len(valid_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea2715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "model_type = \"paper\"\n",
    "if model_type not in model_info:\n",
    "    lr = 0.001\n",
    "    shared = False\n",
    "    constraint = 0.0\n",
    "    scale = 1.0\n",
    "    beta_param = [1.0]\n",
    "    alpha_distribution = Beta(float(beta_param[0]), float(beta_param[0]))\n",
    "    model_params = {}\n",
    "\n",
    "else:\n",
    "    model_params = model_info[model_type]\n",
    "    lr = model_params['lr'] if 'lr' in model_params else 0.001\n",
    "    shared = model_params['shared'] if 'shared' in model_params else False\n",
    "    constraint = model_params['constraint'] if 'constraint' in model_params else 0.0\n",
    "    scale = model_params['scale'] if 'scale' in model_params else 1.0\n",
    "    beta_param = model_params['beta'] if 'beta' in model_params else [1.0]\n",
    "    if len(beta_param) == 1:\n",
    "          alpha_distribution = Beta(float(beta_param[0]), float(beta_param[0]))\n",
    "    elif len(beta_param) == 2:\n",
    "          alpha_distribution = Beta(float(beta_param[0]), float(beta_param[1]))\n",
    "\n",
    "print('model_type: {}\\tscale:{}\\tBeta distribution: Beta({})\\tlr: {}, constraint: {}, seed: {}'\n",
    "    .format(model_type, scale, beta_param, lr, constraint, seed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed to help reproducibility. Please use a fixed initial condition per the pendulum or save the dataset\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e14a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP = True    # Delta value (x(t+1)-x(t)) prediction if True else absolute value (x(t+1)) prediction\n",
    "merge = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeb1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   input_dim = 4\n",
    "input_dim_encoder = 16\n",
    "output_dim_encoder = 64\n",
    "hidden_dim_encoder = 64\n",
    "hidden_dim_db = 64\n",
    "output_dim = input_dim\n",
    "n_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d202e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_encoder = RuleEncoder(input_dim_encoder, output_dim_encoder, hidden_dim=hidden_dim_encoder)\n",
    "data_encoder = DataEncoder(input_dim_encoder, output_dim_encoder, hidden_dim=hidden_dim_encoder)\n",
    "model = SharedNet(input_dim, output_dim, rule_encoder, data_encoder, hidden_dim=hidden_dim_db, n_layers=n_layers, merge=merge, skip=SKIP).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"total parameters: {}\".format(total_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f571d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rule_func = lambda x,y: torch.mean(F.relu(x-y))    # if x>y, penalize it.\n",
    "loss_task_func = nn.L1Loss()    # return scalar (reduction=mean)\n",
    "l1_func = nn.L1Loss()\n",
    "best_val_loss = float('inf')\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5fd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochnum = 500\n",
    "early_stopping_thld = 10\n",
    "counter_early_stopping = 1\n",
    "valid_freq = 5\n",
    "saved_filename = 'dp-{}_{:.4f}_{:.1f}_{:.4f}_{:.1f}-seed{}.skip.demo.pt' \\\n",
    "                      .format(model_type, init_theta1, init_omega1, init_theta2, init_omega2, seed)\n",
    "\n",
    "saved_filename =  os.path.join('saved_models', saved_filename)\n",
    "print('saved_filename: {}\\n'.format(saved_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3bb12",
   "metadata": {},
   "source": [
    "#### The network task\n",
    "The task is to predict the state $(\\theta_1,\\theta_2,\\omega_1,\\omega_2)$ of the system at time $(t + \\Delta t)$ give the state at time $t$. With standard paramenters $\\Delta t$ is 0.1 s so 20 times larger than the $dt$ used for the simulation.\n",
    "A visualization of a traing batch will follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_train_x,batch_train_y = next(iter(train_loader))\n",
    "fig, ax = plt.subplots(figsize=[10, 6])\n",
    "ax.plot(batch_train_x.cpu().numpy()[:,2] + std_noise*np.random.randn(batch_train_x.shape[0]),'o',label=\"elements in a batch of X(t)\")\n",
    "ax.plot(batch_train_y.cpu().numpy()[:,2],'o',label=\"elements in a batch of y (so X(t+dt))\")\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "ax.set_ylabel('radiants', fontsize=24)\n",
    "ax.set_xlabel('batch index', fontsize=24)\n",
    "plt.title(\"Subsampled trajectories\",fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b03ff04",
   "metadata": {},
   "source": [
    "### Training loop \n",
    "#### At each epoch (one iteration over the whole dataset)\n",
    "#### The dataset is divided in mini-batches,at each step we compute the loss and update the wegths for the corrisponding data batch . So for each Step:\n",
    "\n",
    "- data is loaded as a batch\n",
    "- gradients are resetted calling optimizer.zero_grad(). If this function is not called we accumulate gradients from all the calls to loss.backward()\n",
    "\n",
    "To utilize the **Rule Representation** we also need: \n",
    "\n",
    "- alpha $(\\alpha)$ (the rule strength) is sampled from a beta distribution\n",
    "- Energy is computed (mechanical energy of the double pendulum)\n",
    "\n",
    "With the output and the targets we can compute the loss for the task:\n",
    "- Model is called to predict outputs for the current batch (forward pass)\n",
    "- \"Loss Task\" $L_{T}$ is computed (this loss depends only on the data provided for the batch)\n",
    "- \"Loss Rule\" $L_{R}$ is computed (this loss depends only on our knoweledge of the physics involved)\n",
    "\n",
    "For a naive model, the total loss $L_{total}$ would be the sum of $L_{T}$ and $L_{R}$ weigthed with a fixed contrain $\\lambda$\n",
    "\n",
    "$$\n",
    "L_{total} = L_{T} + \\lambda*L_{R}\n",
    "$$\n",
    "\n",
    "Instead this model is trained with a varing weigth $\\alpha$ that is sampled from a given distribution at each step. Moreover a scale parameter $s$ is dinamically computed  to keep $L_{T}$ and $L_{R}$ of the same order of magnitude.\n",
    "\n",
    "\n",
    "$$\n",
    "L_{total} = \\alpha * L_{T} + s * (1-\\alpha)*L_{R}\n",
    "$$\n",
    "\n",
    "With $s =  L_{R}/ L_{T} $\n",
    "\n",
    "With the loss we can now:\n",
    "- compute the the derivative of the loss w.r.t. the parameters (\"the gradients\") using backpropagation calling loss.backward()\n",
    "- call optimizer.step() to make a step based on the gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "ADJ_SCALE=True\n",
    "for epoch in range(1, epochnum+1):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        batch_train_x = batch_data[0] + std_noise*torch.randn(batch_data[0].shape).to(device)    # Adding noise\n",
    "        batch_train_y = batch_data[1]\n",
    "\n",
    "        optimizer.zero_grad() # Clears old gradient from last step. If this function is not called you would accumulate gradients from all the calls loss.backward()\n",
    "    \n",
    "\n",
    " \n",
    "        alpha = alpha_distribution.sample().item()\n",
    "\n",
    "        output = model(batch_train_x, alpha=alpha)\n",
    "\n",
    "        _, _, curr_E = calc_double_E(batch_train_x, **dp_params)    # E(X_t)    Energy of X_t (Current energy)\n",
    "        _, _, next_E = calc_double_E(batch_train_y, **dp_params)    # E(X_{t+1})    Energy of X_{t+1} (Next energy from ground truth)\n",
    "        _, _, pred_E = calc_double_E(output, **dp_params)    # E(\\hat{X}_t+1)    Energy of \\hat{X}_{t+1} (Next energy from prediction)\n",
    "\n",
    "        loss_task = loss_task_func(output, batch_train_y)    # state prediction\n",
    "        loss_rule = loss_rule_func(pred_E, curr_E)    # energy damping by friction: E_{t+1}<=E_t\n",
    "        loss_mae = l1_func(output, batch_train_y).item()\n",
    "\n",
    "        if epoch>5:\n",
    "            if scale == 0:\n",
    "                scale = loss_rule.item() / loss_task.item()\n",
    "                print('scale is updated: {}'.format(scale))\n",
    "        \n",
    "        loss = alpha * loss_rule + scale * (1-alpha) * loss_task\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate on validation set\n",
    "    if epoch % valid_freq == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_task = 0\n",
    "            val_loss_rule = 0\n",
    "            val_ratio = 0\n",
    "            for val_x, val_y in valid_loader:\n",
    "                val_x += 0.01*torch.randn(val_x.shape).to(device)\n",
    "                output = model(val_x, alpha=0.0)\n",
    "                _, _, curr_E = calc_double_E(val_x, **dp_params)\n",
    "                _, _, pred_E = calc_double_E(output, **dp_params)\n",
    "\n",
    "                val_loss_task += (loss_task_func(output, val_y).item() * val_x.shape[0] / total_valid_sample)\n",
    "                val_loss_rule += (loss_rule_func(pred_E, curr_E) * val_x.shape[0] / total_valid_sample)\n",
    "                val_ratio += (verification(curr_E, pred_E, threshold=0.0).item() * val_x.shape[0] / total_valid_sample)\n",
    "\n",
    "                scale_val = val_loss_rule / val_loss_task\n",
    "                if ADJ_SCALE:\n",
    "                    if (torch.abs(scale_val) > 10) | (torch.abs(scale_val) < 0.1):\n",
    "                        scale = val_loss_rule / val_loss_task\n",
    "                        print('scale is updated dinamically from validation: {}'.format(scale))\n",
    "                \n",
    "            if val_loss_task < best_val_loss:\n",
    "                counter_early_stopping = 1\n",
    "                best_val_loss = val_loss_task\n",
    "                print('[Valid] Epoch: {} Loss(Task): {:.6f} Loss(Rule): {:.6f}  Ratio(Rule): {:.3f} Scale_val: {:.3f} (alpha: 0.0)\\t best model is updated %%%%'\n",
    "                          .format(epoch, best_val_loss, val_loss_rule, val_ratio,scale_val))\n",
    "                torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict':model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': best_val_loss\n",
    "                }, saved_filename)\n",
    "            else:\n",
    "                print('[Valid] Epoch: {} Loss(Task): {:.6f} Loss(Rule): {:.6f} Ratio(Rule): {:.3f} Scale_val: {:.3f} (alpha: 0.0) ({}/{})'\n",
    "                  .format(epoch, val_loss_task, val_loss_rule, val_ratio ,scale_val , counter_early_stopping, early_stopping_thld))\n",
    "            if counter_early_stopping >= early_stopping_thld:\n",
    "                break\n",
    "            else:\n",
    "                counter_early_stopping += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4068413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "  \n",
    "rule_encoder = RuleEncoder(input_dim_encoder, output_dim_encoder, hidden_dim=hidden_dim_encoder)\n",
    "data_encoder = DataEncoder(input_dim_encoder, output_dim_encoder, hidden_dim=hidden_dim_encoder)\n",
    "model_eval = SharedNet(input_dim, output_dim, rule_encoder, data_encoder, hidden_dim=hidden_dim_db, n_layers=n_layers, merge=merge, skip=SKIP).to(device)    # delta value prediction\n",
    "\n",
    "checkpoint = torch.load(saved_filename)\n",
    "model_eval.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"best model loss: {:.6f}\\t at epoch: {}\".format(checkpoint['loss'], checkpoint['epoch']))\n",
    "\n",
    "model_eval.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss_task = 0\n",
    "    for test_x, test_y in test_loader:\n",
    "        output = model_eval(test_x, alpha=0.0)\n",
    "        test_loss_task += (loss_task_func(output, test_y).item() * test_x.shape[0] / total_test_sample)  # sum up batch loss\n",
    "\n",
    "print('\\nTest set: Average loss: {:.8f}\\n'.format(test_loss_task))\n",
    "\n",
    "  # Best model\n",
    "alphas = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "val_vs_a = np.zeros(len(alphas))\n",
    "for k,alpha in enumerate(alphas):\n",
    "    model_eval.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss_task, test_ratio = 0, 0\n",
    "        for test_x, test_y in test_loader:\n",
    "\n",
    "            output = model_eval(test_x, alpha=alpha)\n",
    "\n",
    "            test_loss_task += (loss_task_func(output, test_y).item() * test_x.shape[0] / total_test_sample)  # sum up batch loss\n",
    "\n",
    "            _, _, curr_E = calc_double_E(test_x, **dp_params)\n",
    "            _, _, next_E = calc_double_E(test_y, **dp_params)\n",
    "            _, _, pred_E = calc_double_E(output, **dp_params)\n",
    "\n",
    "            test_ratio += (verification(curr_E, pred_E, threshold=0.0).item() * test_x.shape[0] / total_test_sample)\n",
    "        val_vs_a[k]=test_loss_task\n",
    "        print('Test set: Average loss: {:.8f} (alpha:{})'.format(test_loss_task, alpha))\n",
    "        print(\"ratio of verified predictions: {:.6f} (alpha:{})\".format(test_ratio, alpha))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1da4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=[10, 6])\n",
    "plt.plot(alphas,val_vs_a,\"--\",label = \"$\\min loss$\")\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "ax.set_ylabel('LOSS', fontsize=24)\n",
    "ax.set_xlabel('alpha', fontsize=24)\n",
    "plt.title(\"Best apha value for the task\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c125df3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
